---
title: "Course Project Practical Machine Learning"
output: html_document
---
This is a report for the course project of Practical Machine Learning course.

Loading necessary packages
```{r}
library(caret)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(rattle)
library(randomForest)
```

To make the results reproducible, let's set the seed.
```{r}
set.seed(54867)
```

#Loading and cleaning data

In this work firstly we loaded the data, then cleaned it a bit (e.g. cheking for NAs for instance).

```{r}
trainLink <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testLink <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
train <- read.csv(url(trainLink), na.strings=c("NA","#DIV/0!",""))
test <- read.csv(url(testLink), na.strings=c("NA","#DIV/0!",""))
```

Divide the train set into 2 parts (60% and 40%).

```{r}
trainIn <- createDataPartition(y=train$classe, p=0.6, list=FALSE)
myTrain <- train[trainIn, ]
myTest <- train[-trainIn, ]
```

Let's look for near zero variance variables and then create a train set without them.

```{r}
myNZV <- nearZeroVar(myTrain, saveMetrics=TRUE)
myNZVvars <- names(myTrain) %in% c("new_window", "kurtosis_roll_belt", "kurtosis_picth_belt",
"kurtosis_yaw_belt", "skewness_roll_belt", "skewness_roll_belt.1", "skewness_yaw_belt",
"max_yaw_belt", "min_yaw_belt", "amplitude_yaw_belt", "avg_roll_arm", "stddev_roll_arm",
"var_roll_arm", "avg_pitch_arm", "stddev_pitch_arm", "var_pitch_arm", "avg_yaw_arm",
"stddev_yaw_arm", "var_yaw_arm", "kurtosis_roll_arm", "kurtosis_picth_arm",
"kurtosis_yaw_arm", "skewness_roll_arm", "skewness_pitch_arm", "skewness_yaw_arm",
"max_roll_arm", "min_roll_arm", "min_pitch_arm", "amplitude_roll_arm", "amplitude_pitch_arm",
"kurtosis_roll_dumbbell", "kurtosis_picth_dumbbell", "kurtosis_yaw_dumbbell", "skewness_roll_dumbbell",
"skewness_pitch_dumbbell", "skewness_yaw_dumbbell", "max_yaw_dumbbell", "min_yaw_dumbbell",
"amplitude_yaw_dumbbell", "kurtosis_roll_forearm", "kurtosis_picth_forearm", "kurtosis_yaw_forearm",
"skewness_roll_forearm", "skewness_pitch_forearm", "skewness_yaw_forearm", "max_roll_forearm",
"max_yaw_forearm", "min_roll_forearm", "min_yaw_forearm", "amplitude_roll_forearm",
"amplitude_yaw_forearm", "avg_roll_forearm", "stddev_roll_forearm", "var_roll_forearm",
"avg_pitch_forearm", "stddev_pitch_forearm", "var_pitch_forearm", "avg_yaw_forearm",
"stddev_yaw_forearm", "var_yaw_forearm")
myTrain <- myTrain[!myNZVvars]
```

To prevent ID column from interfering with the prediction algorithm, we removed it.

```{r}
myTrain <- myTrain[c(-1)]
```

We didn't use variables (columns) for prediction with the number of NA's more than 60% and identical columns.

```{r}
trainNoNA <- myTrain
for(i in 1:length(myTrain)) { 
        if( sum( is.na( myTrain[, i] ) ) /nrow(myTrain) >= .6 ) { 
        for(j in 1:length(trainNoNA)) {
            if( length( grep(names(myTrain[i]), names(trainNoNA)[j]) ) ==1)  { 
                trainNoNA <- trainNoNA[ , -j] 
            }   
        } 
    }
}
myTrain <- trainNoNA
rm(trainNoNA)
```

Repeat the same cleaning for the test set.

```{r}
cleanFirst <- colnames(myTrain)
cleanSecond <- colnames(myTrain[, -58]) 
myTest <- myTest[cleanFirst]
test <- test[cleanSecond]
```

Finally, let's coerce the data into the same type.

```{r}
for (i in 1:length(test) ) {
        for(j in 1:length(myTrain)) {
        if( length( grep(names(myTrain[i]), names(test)[j]) ) ==1)  {
            class(test[j]) <- class(myTrain[i])
        }      
    }      
}
test <- rbind(myTrain[2, -58] , test) 
test <- test[-1,]
```

#Predictions or ML in practice =)
##Decision Trees

```{r}
mod1 <- rpart(classe ~ ., data=myTrain, method="class")
# plot a fancy plot =)
fancyRpartPlot(mod1)
predict1 <- predict(mod1, myTest, type = "class")
# test the results
confusionMatrix(predict1, myTest$classe)
```


##Random Forests

```{r}
mod2 <- randomForest(classe ~. , data=myTrain)
predict2 <- predict(mod2, myTest, type = "class")
confusionMatrix(predict2, myTest$classe)
```

As we can see Random Forests provide better results than Decision Trees.

#Files to submit

We used better model for an answer.

```{r}
predict2 <- predict(mod2, test, type = "class")
write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

write_files(predict2)
```

The end.